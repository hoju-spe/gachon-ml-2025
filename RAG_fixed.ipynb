{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "#  300ê°œ í•™ìŠµë°ì´í„° ìë™ ìƒì„±ê¸° (JSONL)\n",
        "# ==========================================\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_sample():\n",
        "    prompt = \"\"\"\n",
        "ë‹¤ìŒ í˜•ì‹ì˜ ë©´ì ‘ í•™ìŠµ ë°ì´í„°ë¥¼ í•˜ë‚˜ ìƒì„±í•˜ì„¸ìš”.\n",
        "\n",
        "í˜•ì‹(JSON):\n",
        "{\n",
        " \"instruction\": \"ë©´ì ‘ ì§ˆë¬¸\",\n",
        " \"input\": \"ì›ë³¸ ë‹µë³€\",\n",
        " \"output\": \"STAR ê¸°ë°˜ ê°œì„  ë‹µë³€\"\n",
        "}\n",
        "\n",
        "ì¡°ê±´:\n",
        "- ì§ˆë¬¸ì€ ì‹¤ì œ ë©´ì ‘ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì§ˆë¬¸ì¼ ê²ƒ\n",
        "- ì›ë³¸ ë‹µë³€ì€ â€˜ë¶€ì¡±í•œ/ì§§ì€/êµ¬ì²´ì ì´ì§€ ì•Šì€â€™ ë‹µë³€ì¼ ê²ƒ\n",
        "- output ì€ ì›ë³¸ ë‹µë³€ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ STAR êµ¬ì¡°ë¡œ í™•ì¥\n",
        "- output ì— ìƒˆë¡œìš´ ê²½í—˜ì„ ì°½ì‘í•˜ì§€ ë§ ê²ƒ\n",
        "\"\"\"\n",
        "\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.6\n",
        "    )\n",
        "    txt = res.choices[0].message.content.strip()\n",
        "    return json.loads(txt)\n",
        "\n",
        "dataset = []\n",
        "for i in range(300):\n",
        "    print(f\"{i+1}/300 ìƒì„±ì¤‘...\")\n",
        "    try:\n",
        "        item = generate_sample()\n",
        "        dataset.append(item)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "with open(\"training_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for d in dataset:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\" training_data.jsonl ìƒì„± ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "Ay0VBKCr-xVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb -q\n"
      ],
      "metadata": {
        "id": "JZ4c2mUhMQ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Chroma í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"interview_data\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "# collection ìƒì„±\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"interview_data\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# JSONL ë¡œë“œ\n",
        "data = []\n",
        "with open(\"/content/training_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# DBì— ë„£ê¸°\n",
        "documents = []\n",
        "metadatas = []\n",
        "ids = []\n",
        "\n",
        "for i, item in enumerate(data):\n",
        "    text = f\"\"\"\n",
        "[ë©´ì ‘ ì§ˆë¬¸] {item['instruction']}\n",
        "[ì§§ì€ ë‹µë³€] {item['input']}\n",
        "[STAR í™•ì¥ ë‹µë³€] {item['output']}\n",
        "\"\"\"\n",
        "    documents.append(text)\n",
        "    metadatas.append({\"category\": \"interview\"})\n",
        "    ids.append(str(i))\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(\"300ê°œ ë°ì´í„° ChromaDB ì €ì¥ ì™„ë£Œ!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5NJBlDtMYQn",
        "outputId": "2c9b1754-bc2f-4640-8928-24060fa9cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ 300ê°œ ë°ì´í„° ChromaDB ì €ì¥ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_similar_answers(query, top_k=3):\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    retrieved = []\n",
        "    for doc in results[\"documents\"][0]:\n",
        "        retrieved.append(doc)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(retrieved)\n"
      ],
      "metadata": {
        "id": "WjQHBTsVNZFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install gradio openai transformers accelerate bitsandbytes chromadb -q\n",
        "\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "import chromadb\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  OpenAI API í‚¤\n",
        "# ==========================================\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "client = OpenAI()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  ChromaDB (RAG) ì´ˆê¸°í™”\n",
        "# ==========================================\n",
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"interview_data\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  Qwen 2.5-3B-Instruct ë¡œë“œ (ê°ì • ë¶„ì„ìš©)\n",
        "# ==========================================\n",
        "MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "print(\" Qwen 2.5 3B ëª¨ë¸ ë¡œë”© ì¤‘... (4bit ì–‘ìí™”)\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ ë¶€ë¶„\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": \"cuda\"}  # GPUì— ê°•ì œë¡œ ì˜¬ë¦¼\n",
        ")\n",
        "print(\" ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  RAG ê²€ìƒ‰ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def retrieve_similar_answers(query, top_k=3):\n",
        "    try:\n",
        "        results = collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=top_k\n",
        "        )\n",
        "        docs = results.get(\"documents\", [[]])[0]\n",
        "        return \"\\n\\n---\\n\\n\".join(docs)\n",
        "    except:\n",
        "        return \"(RAG ê²€ìƒ‰ ì‹¤íŒ¨, DB ë¹„ì–´ ìˆìŒ)\"\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  ê°ì • ë¶„ì„ (Qwen 2.5 3B ë¡œì»¬ LLM)\n",
        "# ==========================================\n",
        "def analyze_sentiment_local(text):\n",
        "    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ê°•í•˜ê²Œ ì œì•½ (ì‚¬ì¡± ê¸ˆì§€, ë¬¸ì¥ ì™„ì„±)\n",
        "    system_message = \"\"\"ë‹¹ì‹ ì€ ëƒ‰ì² í•œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì„¸ìš”.\n",
        "ê·œì¹™:\n",
        "1. 'ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤'ê±°ë‚˜ 'ì¶”ì¸¡í•œë‹¤'ëŠ” ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "2. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.\n",
        "3. ê° í•­ëª©ì˜ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ëë‚´ì„¸ìš”.\n",
        "4. ì•„ë˜ í˜•ì‹ì„ ì—„ê²©íˆ ì§€í‚¤ì„¸ìš”.\n",
        "\n",
        "í˜•ì‹:\n",
        "tone: (ê°ì • í†¤)\n",
        "confidence: (ìì‹ ê° ìˆ˜ì¤€)\n",
        "positive: (ê¸ì •ì ì¸ ì )\n",
        "negative: (ì•„ì‰¬ìš´ ì )\n",
        "summary: (ì´í‰)\"\"\"\n",
        "\n",
        "    user_message = f\"ë‹µë³€: {text}\\n\\nìœ„ ë‹µë³€ì„ ë¶„ì„í•´.\"\n",
        "\n",
        "    # Qwenì˜ ì±„íŒ… í…œí”Œë¦¿ ì ìš© (ì§€ì‹œ ì´í–‰ë ¥ ìƒìŠ¹)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "    text_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=512,  # 200 -> 512 (ë¬¸ì¥ ëŠê¹€ ë°©ì§€)\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.3,\n",
        "            repetition_penalty=1.1 # ë°˜ë³µ ë°©ì§€\n",
        "        )\n",
        "\n",
        "    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì˜ë¼ë‚´ê¸°\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  STAR í™•ì¥ (GPT-4o-mini + RAG)\n",
        "# ==========================================\n",
        "def improve_answer_star(question, answer):\n",
        "    rag_context = retrieve_similar_answers(question)\n",
        "\n",
        "    system_prompt = \"ë‹¹ì‹ ì€ STAR/PMI ê¸°ë°˜ì˜ ì „ë¬¸ ë©´ì ‘ ì½”ì¹˜ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "ë©´ì ‘ ì§ˆë¬¸: {question}\n",
        "ì‚¬ìš©ì ë‹µë³€: {answer}\n",
        "\n",
        "[RAG ì°¸ê³  ì‚¬ë¡€]\n",
        "{rag_context}\n",
        "\n",
        "ì•„ë˜ ì¡°ê±´ì— ë§ê²Œ ë©´ì ‘ ë‹µë³€ì„ STAR êµ¬ì¡°ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥í•˜ì„¸ìš”.\n",
        "\n",
        "ì¡°ê±´:\n",
        "- ê²½í—˜ ì°½ì‘ ê¸ˆì§€ (ì‚¬ìš©ì ë‹µë³€ ë²”ìœ„ ë‚´ì—ì„œ í™•ì¥)\n",
        "- STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼) êµ¬ì¡° ìœ ì§€\n",
        "- ë¬¸ì¥ ìˆ˜ 6~10ê°œ\n",
        "- ë©´ì ‘ê´€ì´ ë“£ê¸° ì¢‹ì€ í˜„ì‹¤ì ì¸ ë°©ì‹ìœ¼ë¡œ ì‘ì„±\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  í†µí•© í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def run_all(question, answer):\n",
        "    if not question or not answer:\n",
        "        return \"ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\", \"ì…ë ¥ ëŒ€ê¸° ì¤‘...\"\n",
        "\n",
        "    improved = improve_answer_star(question, answer)     # GPT-4o-mini\n",
        "    sentiment = analyze_sentiment_local(answer)          # Qwen 2.5 3B\n",
        "    return improved, sentiment\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "#  Gradio UI\n",
        "# ==========================================\n",
        "with gr.Blocks(title=\"AI ë©´ì ‘ ìŠ¤í”¼ì¹˜ ì½”ì¹˜ PICO\") as demo:\n",
        "\n",
        "    gr.Markdown(\"# ğŸ¤ AI ë©´ì ‘ ìŠ¤í”¼ì¹˜ ì½”ì¹˜ PICO\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            q_input = gr.Textbox(label=\"ë©´ì ‘ ì§ˆë¬¸\", lines=2, placeholder=\"ì˜ˆ: ë³¸ì¸ì˜ ê°•ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
        "            a_input = gr.Textbox(label=\"ë‚˜ì˜ ë‹µë³€\", lines=10, placeholder=\"ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”.\")\n",
        "            btn = gr.Button(\"âœ¨ ë¶„ì„ ë° ì²¨ì‚­ ë°›ê¸°\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            a_output = gr.Textbox(label=\"AI ìˆ˜ì • ë‹µë³€ (STAR)\", lines=12)\n",
        "            s_output = gr.Textbox(label=\"ë©´ì ‘ê´€ ì‹¬ë¦¬/ê°ì • ë¶„ì„\", lines=12)\n",
        "\n",
        "    btn.click(\n",
        "        fn=run_all,\n",
        "        inputs=[q_input, a_input],\n",
        "        outputs=[a_output, s_output]\n",
        "    )\n",
        "\n",
        "print(\"ğŸŒ Web UI ì‹¤í–‰ ì¤‘â€¦\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737,
          "referenced_widgets": [
            "49d55cc8c830427db85c73573133a41c",
            "b1fda74862ed4c9e8f3d8d52cd957079",
            "530be36b5d564aebb7b916e5466c2d4b",
            "4a161f2ac8934f969aa9cd599716cd8e",
            "61e6e2a076be4573b3cab0599722e04f",
            "75ad5ac4a24d455682a3ff19b1022a9e",
            "d8b8655aa21543e8a369dd185016e3e8",
            "dd5b7b250804477297f34be6dc64bbc3",
            "1e1ed38114a349b592fafa7c24e2cb45",
            "3735318595be4907ac59f9d6a917d83a",
            "c515333786ab42019718a4f2ded902d3"
          ]
        },
        "id": "IdMCFFDY-aRH",
        "outputId": "a9f77ad1-b08c-45aa-fdf6-93f588402ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Qwen 2.5 3B ëª¨ë¸ ë¡œë”© ì¤‘... (4bit ì–‘ìí™”)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49d55cc8c830427db85c73573133a41c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n",
            "ğŸŒ Web UI ì‹¤í–‰ ì¤‘â€¦\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://db29d493283cf21229.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db29d493283cf21229.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://db29d493283cf21229.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}